#!/usr/bin/env python
#
# Example script taken from
# https://huggingface.co/TheBloke/falcon-40b-instruct-3bit-GPTQ
#
import torch
from transformers import AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM
from pathlib import Path

# Download the model from HF and store it locally, then reference its location here:
quantized_model_dir = Path("../falcon-40b-instruct-3bit-GPTQ")

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=False)

model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",
    use_triton=False,
    use_safetensors=True,
    torch_dtype=torch.float32,
    trust_remote_code=True,
)

prompt = "Write a story about llamas"
prompt_template = f"### Instruction: {prompt}\n### Response:"

tokens = tokenizer(prompt_template, return_tensors="pt").to("cuda:0").input_ids
output = model.generate(
    input_ids=tokens, max_new_tokens=100, do_sample=True, temperature=0.8
)
print(tokenizer.decode(output[0]))
